{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's import spaCy and the create_object script.  This includes as `create_object()` function that will generate a generic language object in the folder `new_lang/{language_name}`.  All of the object's files are contained there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.5'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install needed util files if missing\n",
    "import spacy \n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !mkdir util\n",
    "    !wget -O /content/util/corpus.py https://raw.githubusercontent.com/New-Languages-for-NLP/cadet-the-notebook/main/util/corpus.py\n",
    "    !wget -O /content/util/create_object.py https://raw.githubusercontent.com/New-Languages-for-NLP/cadet-the-notebook/main/util/create_object.py\n",
    "    !wget -O /content/util/export.py https://raw.githubusercontent.com/New-Languages-for-NLP/cadet-the-notebook/main/util/export.py\n",
    "    !wget -O /content/util/tokenization.py https://raw.githubusercontent.com/New-Languages-for-NLP/cadet-the-notebook/main/util/tokenization.py\n",
    "    #colab currently uses spacy 2.2.4, need 3\n",
    "    if '3' not in spacy.__version__[:1]:\n",
    "        !pip install spacy --upgrade\n",
    "\n",
    "import spacy\n",
    "from util.create_object import create_object\n",
    "spacy.__version__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üçà created language object for meow'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_name = 'Meow'\n",
    "lang_code ='meow'\n",
    "direction = 'ltr' #or 'rtl'\n",
    "has_case = True\n",
    "has_letters = True\n",
    "\n",
    "create_object(lang_name, lang_code, direction, has_case, has_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_config.cfg  lemmatizer.py\tpunctuation.py\tsyntax_iterators.py\r\n",
      "corpus_json\t lex_attrs.py\t__pycache__\ttag_map.py\r\n",
      "examples.py\t lookups\tsetup.py\ttexts\r\n",
      "__init__.py\t meow.egg-info\tstop_words.py\ttokenizer_exceptions.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./new_lang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess how the tokenizer defaults will work with your language, add example sentences to the [`examples.py`](./new_lang/examples.py) file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><span style='border: 5px solid blue; margin:5px;'>There</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>I</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>was</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>,</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>sitting</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>on</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>a</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>stool</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>.</span>&nbsp;</div><div><span style='border: 5px solid blue; margin:5px;'>But</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>I</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>never</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>got</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>the</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>name</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>of</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>that</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>couragous</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>murman</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>.</span>&nbsp;</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "from util.tokenization import tokenization\n",
    "HTML(tokenization(lang_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To adjust the tokenizer you can add unique exceptions or regular exceptions to the [tokenizer_exceptions.py](./new_lang/tokenizer_exceptions.py) file\n",
    "\n",
    "- To join two tokens, add an exception `{'BIG YIKES':[{ORTH: 'BIG YIKES'}]}`\n",
    "- To split a token in two, `{'Kummerspeck':[{ORTH:\"Kummer\"},{ORTH:\"speck\"}]}`\n",
    "\n",
    "Note in both cases that we add a dictionary. The key is the string to match on, with a list of tokens.  In the first case we had a single token where we would otherwise have two and vice versa. You can find more details in the spaCy documentation and [here](https://new-languages-for-nlp.github.io/course-materials/w1/tokenization.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookups \n",
    "\n",
    "The `create_object()` function creates a `new_lang/lookups` directory that contains three files.  These are simple json lookups for unambiguous pos, lemma and entities. You can add your data to these files and automatically update token values.  Keep in mind that you'll need to find a balance between the convenience of automatically annotating tokens and the inconvenience of having to correct machine errors.  Once you're done updating the files with your existing linguistic data, proceed to the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts\n",
    "\n",
    "For us to identify frequent tokens for automatic annotation, you'll need to provide texts.  Place your machine-readable utf-8 text files in the `new_lang/texts` folder.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texts': 5, 'tokens': 570558, 'unique_tokens': 57166}\n"
     ]
    }
   ],
   "source": [
    "from util.corpus import make_corpus\n",
    "\n",
    "make_corpus(lang_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of make_corpus is a json file at [`new_lang/corpus_json/tokens.json`](./new_lang/corpus_json/tokens.json). For each token, you'll find a `text` key for the token's string as well as keys for pos_, lemma_ and ent_type_. Keep in mind that this system is not able to process ambiguous lookups.  Only enter data for tokens or spans with very little semantic variation.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üçâ To bulk annotate 33% of the corpus, add data to first 31 tokens\n",
      "üçÖ To bulk annotate 50% of the corpus, add data to first 137 tokens\n",
      "üçí To bulk annotate 66% of the corpus, add data to first 730 tokens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import srsly\n",
    "from pathlib import Path \n",
    "\n",
    "def get_percentages():\n",
    "    thirds = []\n",
    "    halfs = []\n",
    "    two_thirds = [] \n",
    "    tokens = srsly.read_json(Path.cwd() / 'new_lang' / 'corpus_json' / 'tokens.json')\n",
    "    tokens = srsly.json_loads(tokens)\n",
    "    for token in tokens:\n",
    "        if token['rank'] == 1:\n",
    "            total_tokens = token['count'] + token['remain']\n",
    "        \n",
    "        percent_annotated = 1 - (token['remain'] / total_tokens)\n",
    "        percent_annotated = int((percent_annotated * 100))\n",
    "        if percent_annotated == 33:\n",
    "            thirds.append(token)\n",
    "        if percent_annotated == 50:\n",
    "            halfs.append(token)\n",
    "        if percent_annotated == 66:\n",
    "            two_thirds.append(token)\n",
    "    return thirds[0], halfs[0], two_thirds[0]\n",
    "        \n",
    "    #let percent_annotated = 1 - (token.remain / total_tokens);\n",
    "#    let percent_annotated_str = (percent_annotated*100).toFixed(0);\n",
    "third, half, two_thirds = get_percentages()\n",
    "print(f\"\"\"\n",
    "üçâ To bulk annotate 33% of the corpus, add data to first {third['rank']} tokens\n",
    "üçÖ To bulk annotate 50% of the corpus, add data to first {half['rank']} tokens\n",
    "üçí To bulk annotate 66% of the corpus, add data to first {two_thirds['rank']} tokens\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will export your texts and lookups in a TSV file in the CoreNLP format.  This data can then be loaded into INCEpTION for annotation work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved data to file /tmp/conll_export.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util.export import download\n",
    "\n",
    "download(lang_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have completed all annotation work in INCEpTION, you're ready to begin model training. This final step will export your spaCy language object. From there you can follow the spaCy documentation on model training!  \n",
    "\n",
    "1. package the object into a usable folder, that can be moved, and initialized using projects\n",
    "2. nlp.to_disk(\"/tmp/checkpoint\")?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-6eaab29b208e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# If not on Vocab, add it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#string_id = nlp.vocab.strings[match_id]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mupdate_vocab_from_lookups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-6eaab29b208e>\u001b[0m in \u001b[0;36mupdate_vocab_from_lookups\u001b[0;34m(nlp)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#update lexemes aready in Vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemma_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "# You can save the language object to disk\n",
    "from util.export import get_nlp\n",
    "nlp = get_nlp(lang_name)\n",
    "#TODO update vocab from lookups (will then save to Vocab)\n",
    "def update_vocab_from_lookups(nlp):\n",
    "\n",
    "    #Read the lookups directory, make dict of table names and path to json files\n",
    "    new_lang = Path.cwd() / \"new_lang\"\n",
    "    lookups_path = new_lang / \"lookups\"\n",
    "    for lookup in lookups_path.iterdir():\n",
    "        key = lookup.stem[lookup.stem.find('_') + 1:]\n",
    "        if 'lemma' in key:\n",
    "            lemma_data = srsly.read_json(lookup)\n",
    "            assert isinstance(lemma_data, dict)\n",
    "\n",
    "        if 'pos' in key:\n",
    "            pos_data = srsly.read_json(lookup)\n",
    "            assert isinstance(pos_data, dict)\n",
    "\n",
    "    #update lexemes aready in Vocab\n",
    "    for lex in nlp.vocab:\n",
    "        lemma = lemma_data.get(lex.text, None)\n",
    "        if lemma:\n",
    "            lex.text = lemma\n",
    "        pos = pos_data.get(t.text, None)\n",
    "        if pos:\n",
    "            try:\n",
    "                t.pos_ = pos\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "\n",
    "    nlp.vocab.lookups # https://spacy.io/api/lookups#_title\n",
    "         \"pos_lookup\"  \n",
    "        'lemma_lookup'\n",
    "    # If not on Vocab, add it \n",
    "            #string_id = nlp.vocab.strings[match_id]\n",
    "update_vocab_from_lookups(nlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(f'/tmp/{lang_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.cfg  meta.json  tokenizer  vocab\r\n"
     ]
    }
   ],
   "source": [
    "#https://spacy.io/api/language#serialization-fields\n",
    "!ls /tmp/Meow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "nlp = Language().from_disk('/tmp/Meow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Meow' object has no attribute 'examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5ffc894c0323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Meow' object has no attribute 'examples'"
     ]
    }
   ],
   "source": [
    "nlp.examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_lang import Meow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lookups.Lookups at 0x7f5a870541f0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['√º.',\n",
       " 'XD',\n",
       " '333',\n",
       " ';-D',\n",
       " '>:o',\n",
       " ':-)',\n",
       " '<3',\n",
       " ' ',\n",
       " '8)',\n",
       " '\\\\n',\n",
       " 'c.',\n",
       " 'd',\n",
       " ':>',\n",
       " 'b.',\n",
       " ':-}',\n",
       " '>',\n",
       " '(¬¨_¬¨)',\n",
       " 'v',\n",
       " 'd.',\n",
       " ':O',\n",
       " 'f',\n",
       " 'o.O',\n",
       " 'V_V',\n",
       " '‚ïØ',\n",
       " ':(((',\n",
       " ':0',\n",
       " '=3',\n",
       " ']',\n",
       " '-8',\n",
       " '>.>',\n",
       " 'c',\n",
       " '(^_^)',\n",
       " ':}',\n",
       " 'e',\n",
       " 's',\n",
       " ':D',\n",
       " 'm',\n",
       " ':-o',\n",
       " 'n.',\n",
       " 'p.',\n",
       " '-/',\n",
       " 'm.',\n",
       " '>.<',\n",
       " '^__^',\n",
       " 'p',\n",
       " '8-',\n",
       " ':))',\n",
       " '0_0',\n",
       " '¬¨_¬¨',\n",
       " 'j',\n",
       " ':-*',\n",
       " '=',\n",
       " 'g.',\n",
       " ':()',\n",
       " ')-:',\n",
       " 'X',\n",
       " 'y',\n",
       " '0.o',\n",
       " ':o',\n",
       " '>:(',\n",
       " ':|',\n",
       " '<.<',\n",
       " '\\\\\")',\n",
       " '</3',\n",
       " '‚îÅ',\n",
       " ':1',\n",
       " '(-_-)',\n",
       " '¬∞',\n",
       " ':‚Äô(',\n",
       " '=|',\n",
       " '^___^',\n",
       " '0.0',\n",
       " 'v_v',\n",
       " '0',\n",
       " ':‚Äô-(',\n",
       " 'x.',\n",
       " '><(((*>',\n",
       " '-o',\n",
       " ':‚Äô)',\n",
       " '(>_<)',\n",
       " '=D',\n",
       " '-0',\n",
       " \":'(\",\n",
       " '8',\n",
       " '.',\n",
       " '8-)',\n",
       " '(=',\n",
       " '-O',\n",
       " '<33',\n",
       " 'k.',\n",
       " 'n',\n",
       " 'o_O',\n",
       " '\"',\n",
       " ':o)',\n",
       " '}',\n",
       " ':-3',\n",
       " 'b',\n",
       " '(*_*)',\n",
       " '(‚ïØ¬∞‚ñ°¬∞Ôºâ‚ïØÔ∏µ‚îª‚îÅ‚îª',\n",
       " '\\\\',\n",
       " 'O.o',\n",
       " '3',\n",
       " 'q.',\n",
       " ':/',\n",
       " '‡≤†_‡≤†',\n",
       " 'h',\n",
       " '‚îª',\n",
       " 'o_0',\n",
       " '0_o',\n",
       " 'x',\n",
       " '\\xa0',\n",
       " '‚Äô‚Äô',\n",
       " '[:',\n",
       " '[=',\n",
       " '√§.',\n",
       " 'o',\n",
       " '-__-',\n",
       " \":'-)\",\n",
       " '√∂.',\n",
       " '<333',\n",
       " '\\t',\n",
       " '(-;',\n",
       " ':p',\n",
       " '<space>',\n",
       " 'y.',\n",
       " '(o:',\n",
       " 'h.',\n",
       " '=]',\n",
       " ':-(',\n",
       " '@_@',\n",
       " ':-((',\n",
       " '<',\n",
       " 'v.v',\n",
       " ':3',\n",
       " 'o.o',\n",
       " '-P',\n",
       " '‡≤†Ô∏µ‡≤†',\n",
       " 'space',\n",
       " '¬Ø\\\\(„ÉÑ)/¬Ø',\n",
       " '√§',\n",
       " '33',\n",
       " ':X',\n",
       " ';)',\n",
       " \"''\",\n",
       " ':-(((',\n",
       " ';D',\n",
       " 'i.',\n",
       " 'i',\n",
       " 't',\n",
       " '-X',\n",
       " 'xD',\n",
       " '=)',\n",
       " 'a',\n",
       " 't.',\n",
       " '8-D',\n",
       " ':-D',\n",
       " '[',\n",
       " 'z.',\n",
       " ':-X',\n",
       " '‚Äô',\n",
       " 'v.',\n",
       " '√º',\n",
       " '-x',\n",
       " 'r',\n",
       " 'Ô∏µ',\n",
       " '/3',\n",
       " ':-))',\n",
       " ':)',\n",
       " ';_;',\n",
       " 'z',\n",
       " '*',\n",
       " '-_-',\n",
       " '(‡≤†_‡≤†)',\n",
       " '\\\\t',\n",
       " 'D',\n",
       " ':x',\n",
       " '1',\n",
       " ']=',\n",
       " 'o_o',\n",
       " '/',\n",
       " 'O',\n",
       " ':(',\n",
       " '8D',\n",
       " ':->',\n",
       " 'j.',\n",
       " '(._.)',\n",
       " '._.',\n",
       " ':‚Äô-)',\n",
       " ':-)))',\n",
       " ':-0',\n",
       " 'Ôºâ',\n",
       " '-p',\n",
       " '|',\n",
       " 'O_o',\n",
       " ':((',\n",
       " 'o.0',\n",
       " '√∂',\n",
       " '):',\n",
       " 'O_O',\n",
       " 'r.',\n",
       " '(-:',\n",
       " ':',\n",
       " 'xDD',\n",
       " ')',\n",
       " 'l.',\n",
       " 'XDD',\n",
       " 'P',\n",
       " '-|',\n",
       " '=/',\n",
       " '^_^',\n",
       " '[-:',\n",
       " 'g',\n",
       " 's.',\n",
       " 'C++',\n",
       " '(:',\n",
       " ':-x',\n",
       " '-D',\n",
       " '-',\n",
       " '(',\n",
       " 'u.',\n",
       " '(-8',\n",
       " 'w.',\n",
       " ':-p',\n",
       " ':-]',\n",
       " '(;',\n",
       " 'f.',\n",
       " '=(',\n",
       " ':]',\n",
       " '_',\n",
       " '‚ñ°',\n",
       " 'O.O',\n",
       " '-3',\n",
       " ':P',\n",
       " '=[',\n",
       " 'V.V',\n",
       " 'e.',\n",
       " 'w',\n",
       " \":'-(\",\n",
       " \"'\",\n",
       " 'l',\n",
       " ':-P',\n",
       " '‚Äî',\n",
       " ':-/',\n",
       " '\\n',\n",
       " ';',\n",
       " 'u',\n",
       " 'a.',\n",
       " ':)))',\n",
       " ':-|',\n",
       " ':*',\n",
       " \":')\",\n",
       " ':-O',\n",
       " 'k',\n",
       " 'o.',\n",
       " 'q',\n",
       " ';-)']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = (lex for lex in nlp.vocab)\n",
    "[w.text for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaCy v3",
   "language": "python",
   "name": "spacy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
