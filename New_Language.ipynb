{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's import spaCy and the create_object script.  This includes as `create_object()` function that will generate a generic language object in the folder `new_lang/{language_name}`.  All of the object's files are contained there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from util.create_object import create_object\n",
    "spacy.__version__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('meow', 'meow')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_name = 'Meow'\n",
    "lang_code ='meow'\n",
    "direction = 'ltr' #or 'rtl'\n",
    "has_case = True\n",
    "has_letters = True\n",
    "\n",
    "create_object(lang_name, lang_code, direction, has_case, has_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_config.cfg  lex_attrs.py\tpunctuation.py\tsyntax_iterators.py\r\n",
      "examples.py\t lookups\t__pycache__\ttag_map.py\r\n",
      "__init__.py\t Meow\t\tsetup.py\ttexts\r\n",
      "lemmatizer.py\t meow.egg-info\tstop_words.py\ttokenizer_exceptions.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./new_lang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess how the tokenizer defaults will work with your language, add example sentences to the [`examples.py`](./new_lang/examples.py) file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><span style='border: 5px solid blue; margin:5px;'>This</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>was</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>sentence</span>&nbsp;</div><div><span style='border: 5px solid blue; margin:5px;'>Then</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>more</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>sentence</span>&nbsp;<span style='border: 5px solid blue; margin:5px;'>.</span>&nbsp;</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "from util.tokenization import tokenization\n",
    "HTML(tokenization(lang_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To adjust the tokenizer you can add unique exceptions or regular exceptions to the [tokenizer_exceptions.py](./new_lang/tokenizer_exceptions.py) file\n",
    "\n",
    "- To join two tokens, add an exception `{'BIG YIKES':[{ORTH: 'BIG YIKES'}]}`\n",
    "- To split a token in two, `{'Kummerspeck':[{ORTH:\"Kummer\"},{ORTH:\"speck\"}]}`\n",
    "\n",
    "Note in both cases that we add a dictionary. The key is the string to match on, with a list of tokens.  In the first case we had a single token where we would otherwise have two and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookups \n",
    "\n",
    "The `create_object()` function creates a `new_lang/lookups` directory that contains three files.  These are simple json lookups for unambiguous pos, lemma and entities. You can add your data to these files and automatically update token values.  Keep in mind that you'll need to find a balance between the convenience of automatically annotating tokens and the inconvenience of having to correct machine errors.  Once you're done updating the files with your existing linguistic data, proceed to the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts\n",
    "\n",
    "For us to identify frequent tokens for automatic annotation, you'll need to provide texts.  Place your machine-readable utf-8 text files in the `new_lang/texts` folder.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texts': 4, 'tokens': 409276, 'unique_tokens': 43313}\n"
     ]
    }
   ],
   "source": [
    "from util.corpus import make_corpus\n",
    "\n",
    "make_corpus(lang_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaCy v3",
   "language": "python",
   "name": "spacy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
